{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO算法：理论与代码逐块对应\n",
    "\n",
    "本Notebook将PPO (Proximal Policy Optimization) 的核心公式与代码实现逐块对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 概率比 r(θ)\n",
    "\n",
    "### 公式\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$$\n",
    "\n",
    "### 对数形式（数值稳定）\n",
    "$$r_t = \\exp(\\log \\pi_\\theta - \\log \\pi_{\\text{old}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratio(new_log_probs, old_log_probs):\n",
    "    \"\"\"\n",
    "    计算概率比 r(θ) = π_new / π_old\n",
    "    \n",
    "    使用对数计算更稳定：\n",
    "    r = exp(log π_new - log π_old)\n",
    "    \"\"\"\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    return ratio\n",
    "\n",
    "# 示例\n",
    "old_log_prob = torch.tensor(-1.0)  # log π_old(a|s) = -1.0\n",
    "new_log_prob = torch.tensor(-0.8)  # log π_new(a|s) = -0.8 (概率更高了)\n",
    "\n",
    "ratio = compute_ratio(new_log_prob, old_log_prob)\n",
    "print(f\"log π_old = {old_log_prob.item():.2f} → π_old = {np.exp(old_log_prob.item()):.4f}\")\n",
    "print(f\"log π_new = {new_log_prob.item():.2f} → π_new = {np.exp(new_log_prob.item()):.4f}\")\n",
    "print(f\"概率比 r = π_new/π_old = {ratio.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. PPO-Clip目标函数\n",
    "\n",
    "### 公式\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t A_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]$$\n",
    "\n",
    "### 公式分解\n",
    "1. 原始目标：$r_t \\cdot A_t$\n",
    "2. 裁剪目标：$\\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) \\cdot A_t$\n",
    "3. 取较小者：$\\min(\\text{原始}, \\text{裁剪})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_clip_objective(old_log_probs, new_log_probs, advantages, clip_epsilon=0.2):\n",
    "    \"\"\"\n",
    "    PPO-Clip目标函数\n",
    "    \n",
    "    L = min(r·A, clip(r, 1-ε, 1+ε)·A)\n",
    "    \"\"\"\n",
    "    # 步骤1: 计算概率比\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # 步骤2: 原始目标 r * A\n",
    "    surr1 = ratio * advantages\n",
    "    \n",
    "    # 步骤3: 裁剪目标 clip(r, 1-ε, 1+ε) * A\n",
    "    clipped_ratio = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon)\n",
    "    surr2 = clipped_ratio * advantages\n",
    "    \n",
    "    # 步骤4: 取min (转为loss需要加负号)\n",
    "    objective = torch.min(surr1, surr2)\n",
    "    \n",
    "    return objective, ratio, clipped_ratio\n",
    "\n",
    "# 示例：好动作 (A > 0)\n",
    "old_lp = torch.tensor([-0.5, -0.5, -0.5])\n",
    "new_lp = torch.tensor([-0.3, -0.5, -0.8])  # 概率增/不变/减\n",
    "advantages = torch.tensor([1.0, 1.0, 1.0])  # 正优势\n",
    "\n",
    "obj, r, r_clip = ppo_clip_objective(old_lp, new_lp, advantages)\n",
    "print(\"好动作 (A > 0):\")\n",
    "print(f\"  概率比 r     = {r.numpy()}\")\n",
    "print(f\"  裁剪后 r_clip = {r_clip.numpy()}\")\n",
    "print(f\"  目标 L       = {obj.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Actor-Critic网络\n",
    "\n",
    "### 结构\n",
    "```\n",
    "状态 s → [共享层] → [Actor头] → 动作概率 π(a|s)\n",
    "                 ↘ [Critic头] → 状态价值 V(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Actor-Critic网络\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden=64):\n",
    "        super().__init__()\n",
    "        # 共享特征层\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # Actor: 输出动作logits\n",
    "        self.actor = nn.Linear(hidden, action_dim)\n",
    "        # Critic: 输出状态价值\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.shared(state)\n",
    "        action_logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return action_logits, value.squeeze(-1)\n",
    "\n",
    "# 测试\n",
    "net = ActorCritic(state_dim=4, action_dim=2)\n",
    "state = torch.randn(1, 4)\n",
    "logits, value = net(state)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(f\"状态: {state.numpy().flatten()[:2]}...\")\n",
    "print(f\"动作概率: {probs.detach().numpy().flatten()}\")\n",
    "print(f\"状态价值: {value.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GAE (广义优势估计)\n",
    "\n",
    "### 公式\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) \\quad \\text{(TD误差)}$$\n",
    "$$\\hat{A}_t = \\delta_t + \\gamma\\lambda \\hat{A}_{t+1} \\quad \\text{(GAE递归)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
    "    \"\"\"\n",
    "    计算GAE优势\n",
    "    \n",
    "    从后往前递归：\n",
    "    δ_t = r_t + γ·V(s') - V(s)\n",
    "    A_t = δ_t + γλ·A_{t+1}\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = [0.0] * T\n",
    "    gae = 0\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        if t == T - 1:\n",
    "            next_value = 0  # 终止状态\n",
    "        else:\n",
    "            next_value = values[t + 1]\n",
    "        \n",
    "        # TD误差\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        \n",
    "        # GAE递归\n",
    "        gae = delta + gamma * gae_lambda * gae\n",
    "        advantages[t] = gae\n",
    "    \n",
    "    return advantages\n",
    "\n",
    "# 示例\n",
    "rewards = [1, 1, 1, 1, 1]\n",
    "values = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "advantages = compute_gae(rewards, values)\n",
    "\n",
    "print(\"时刻 | 奖励 | V(s) | 优势 A\")\n",
    "print(\"-\" * 35)\n",
    "for t, (r, v, a) in enumerate(zip(rewards, values, advantages)):\n",
    "    print(f\"  {t}  |  {r}   | {v:.1f}  | {a:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 完整PPO更新\n",
    "\n",
    "### 损失函数\n",
    "$$L = L^{CLIP} + c_1 L^{VF} - c_2 S[\\pi]$$\n",
    "\n",
    "其中：\n",
    "- $L^{CLIP}$: 策略损失（裁剪目标的负值）\n",
    "- $L^{VF} = (V_\\theta(s) - V_{target})^2$: 价值损失\n",
    "- $S[\\pi]$: 熵正则化（鼓励探索）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(old_log_probs, new_log_probs, advantages, \n",
    "             values, returns, entropy,\n",
    "             clip_epsilon=0.2, value_coef=0.5, entropy_coef=0.01):\n",
    "    \"\"\"\n",
    "    PPO总损失\n",
    "    \n",
    "    L = L_policy + c1 * L_value - c2 * entropy\n",
    "    \"\"\"\n",
    "    # 策略损失\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    # 价值损失\n",
    "    value_loss = F.mse_loss(values, returns)\n",
    "    \n",
    "    # 熵正则化（负号因为要最大化熵）\n",
    "    entropy_loss = -entropy.mean()\n",
    "    \n",
    "    # 总损失\n",
    "    total_loss = policy_loss + value_coef * value_loss + entropy_coef * entropy_loss\n",
    "    \n",
    "    return total_loss, policy_loss, value_loss, entropy.mean()\n",
    "\n",
    "print(\"PPO损失函数定义完成！\")\n",
    "print(\"损失组成:\")\n",
    "print(\"  - 策略损失: -E[min(r·A, clip(r)·A)]\")\n",
    "print(\"  - 价值损失: MSE(V, V_target)\")\n",
    "print(\"  - 熵正则化: -c2 * H(π)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "| 公式 | 代码 |\n",
    "|------|------|\n",
    "| $r = \\pi/\\pi_{old}$ | `torch.exp(log_new - log_old)` |\n",
    "| $\\text{clip}(r, 1\\pm\\epsilon)$ | `torch.clamp(r, 1-ε, 1+ε)` |\n",
    "| $L^{CLIP}$ | `-torch.min(r*A, clip(r)*A).mean()` |\n",
    "| GAE | `delta + γλ * gae` (递归) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
