---
title: PPO vs GRPO - Reinforcement Learning for LLMs
date: 2026-02-05
author: Gemini
tags:
  - Reinforcement Learning
  - PPO
  - GRPO
  - LLM
  - DeepSeek
categories:
  - AI Research
---

# PPO vs GRPO: Demystifying Reinforcement Learning for LLMs

> Based on the research from [PPO-GRPO Repository](https://github.com/SingularGuyLeBorn/PPO-GRPO).

In the rapidly evolving landscape of Large Language Models (LLMs), Reinforcement Learning (RL) plays a pivotal role in aligning models with human intent. Two prominent algorithms in this space are **Proximal Policy Optimization (PPO)** and **Group Relative Policy Optimization (GRPO)**. This article dives deep into their mechanics, implementation details, and differences.

## 1. Proximal Policy Optimization (PPO)

PPO has been the gold standard for RLHF (Reinforcement Learning from Human Feedback). It is reliable, stable, and widely adopted (e.g., by OpenAI for InstructGPT).

### Core Mechanism

PPO uses a "trust region" approach to ensure that the new policy does not deviate too wildly from the old policy during a single update step. This is achieved via a clipped objective function.

$$
L^{CLIP}(\theta) = \hat{E}_t [\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
$$

Where:
- $r_t(\theta)$ is the probability ratio between the new and old policy.
- $\hat{A}_t$ is the estimated advantage.
- $\epsilon$ is a hyperparameter (usually 0.1 or 0.2) that limits the update.

### Pros & Cons
- **Pros**: Stable, principled, prevents catastrophic forgetting.
- **Cons**: Computationally expensive due to the need for a separate **Value Model (Critic)** which effectively doubles the memory footprint during training.

## 2. Group Relative Policy Optimization (GRPO)

GRPO, popularized by DeepSeek's research (DeepSeek-R1), offers a more efficient alternative by eliminating the need for a Value Model.

### How GRPO Works

Instead of using a Critic model to estimate the value of a state, GRPO samples a **group of outputs** (e.g., $G$ outputs) for the same prompt and uses the group mean to calculate the baseline.

The advantage for each output in the group is calculated as:

$$
A_i = \frac{r_i - \text{mean}(\{r_1, ..., r_G\})}{\text{std}(\{r_1, ..., r_G\}) + \epsilon}
$$

This method normalizes the rewards within the sampled group, using the group average as the baseline expectation.

### Advantages of GRPO
1.  **Memory Efficiency**: No need to load a separate Critic model (often as large as the Policy model). This saves massive amounts of VRAM.
2.  **Simplicity**: Streamlines the training loop by removing the Value Function loss calculation.
3.  **Effectiveness**: As shown in DeepSeek-Math and DeepSeek-R1, GRPO scales exceptionally well for reasoning tasks where there is a clear "correct" answer or verifiable outcome.

## 3. Implementation Insights

### PPO Trainer Structure
The standard PPO trainer usually involves four models:
1.  **Actor (Policy)**: The model being trained.
2.  **Ref_Model (Reference)**: Frozen copy for KL divergence calculation.
3.  **Critic (Value)**: Estimates state values.
4.  **Reward Model**: Provides the scalar reward signal.

### GRPO Trainer Structure
By contrast, GRPO simplifies this to:
1.  **Actor (Policy)**: The model being trained.
2.  **Ref_Model (Reference)**: Frozen copy for KL.
3.  **Reward Model**: (Or even rule-based verifiers for math/code tasks).

### Code Snippet: GRPO Loss Calculation

```python
def compute_grpo_loss(prompts, completions, rewards, old_policy_probs):
    # 1. Compute Advantage using Group Relative normalization
    mean_rewards = rewards.mean(dim=-1, keepdim=True)
    std_rewards = rewards.std(dim=-1, keepdim=True)
    advantages = (rewards - mean_rewards) / (std_rewards + 1e-8)
    
    # 2. Compute Ratio (Importance Sampling)
    # ... standard policy gradient logic ...
    
    # 3. Compute KL Divergence Penalty
    # ... ensure model stays close to reference ...
    
    return loss
```

## 4. Conclusion

While PPO remains a robust general-purpose RL algorithm, GRPO represents a significant leap forward for training LLMs, especially in resource-constrained environments or when specializing in reasoning tasks. The removal of the Critic model allows for larger batch sizes or larger base models on the same hardware.

---
*Generated by Gemini for the MetaUniverse Blog.*
