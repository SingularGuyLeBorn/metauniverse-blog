{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAPO算法：理论与代码逐块对应\n",
    "\n",
    "本Notebook将DAPO的四大技术与代码实现逐块对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Clip-Higher (解耦裁剪)\n",
    "\n",
    "### 公式\n",
    "$$\\text{clip}^{DAPO}(r, A) = \\begin{cases}\n",
    "\\min(r, 1+\\epsilon_{high}) & A > 0 \\text{ (只裁上界)}\\\\\n",
    "\\max(r, 1-\\epsilon_{low}) & A < 0 \\text{ (只裁下界)}\n",
    "\\end{cases}$$\n",
    "\n",
    "DAPO使用 $\\epsilon_{high}=0.28 > \\epsilon_{low}=0.2$，鼓励探索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dapo_clip(ratio, advantages, eps_high=0.28, eps_low=0.2):\n",
    "    \"\"\"\n",
    "    DAPO解耦裁剪\n",
    "    \n",
    "    A > 0: 只裁上界 (允许更多探索)\n",
    "    A < 0: 只裁下界 (防止过度惩罚)\n",
    "    \"\"\"\n",
    "    clipped = torch.where(\n",
    "        advantages > 0,\n",
    "        torch.clamp(ratio, max=1 + eps_high),\n",
    "        torch.clamp(ratio, min=1 - eps_low)\n",
    "    )\n",
    "    return clipped\n",
    "\n",
    "# 演示\n",
    "ratio = torch.tensor([0.7, 0.9, 1.1, 1.5])\n",
    "advantages = torch.tensor([1.0, 1.0, -1.0, -1.0])\n",
    "\n",
    "clipped = dapo_clip(ratio, advantages)\n",
    "print(f\"概率比 r:  {ratio.tolist()}\")\n",
    "print(f\"优势 A:    {advantages.tolist()}\")\n",
    "print(f\"裁剪后:    {clipped.tolist()}\")\n",
    "print()\n",
    "print(\"解读:\")\n",
    "print(\"  - r=0.7, A>0: 不裁剪 (下界不影响正优势)\")\n",
    "print(\"  - r=1.5, A<0: 不裁剪 (上界不影响负优势)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Dynamic Sampling (动态采样)\n",
    "\n",
    "过滤全零奖励的prompts，避免无效更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_zero_prompts(rewards, group_size):\n",
    "    \"\"\"过滤全零奖励的prompts\"\"\"\n",
    "    rewards_grouped = rewards.view(-1, group_size)\n",
    "    has_nonzero = (rewards_grouped != 0).any(dim=1)\n",
    "    return has_nonzero\n",
    "\n",
    "# 演示: 3个prompts，每个4个responses\n",
    "rewards = torch.tensor([\n",
    "    1.0, 0.0, 0.5, 0.0,  # Prompt 1: 有非零 ✓\n",
    "    0.0, 0.0, 0.0, 0.0,  # Prompt 2: 全零 ✗\n",
    "    0.0, 1.0, 0.0, 1.0   # Prompt 3: 有非零 ✓\n",
    "])\n",
    "\n",
    "valid = filter_zero_prompts(rewards, group_size=4)\n",
    "print(f\"奖励: {rewards.tolist()}\")\n",
    "print(f\"有效prompts: {valid.tolist()}\")\n",
    "print(\"\\n结果: Prompt 2被过滤，避免无效更新\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Token-Level Loss\n",
    "\n",
    "### 公式\n",
    "$$L^{DAPO} = -\\sum_t \\text{clip}(r_t) \\cdot A$$\n",
    "\n",
    "逐token计算损失，更细粒度的信用分配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_level_loss(per_token_ratio, advantages, mask):\n",
    "    \"\"\"\n",
    "    Token级损失\n",
    "    \n",
    "    L = -Σ_t r_t · A\n",
    "    \"\"\"\n",
    "    # 扩展优势到token维度\n",
    "    adv_expanded = advantages.unsqueeze(-1)  # [B, 1]\n",
    "    \n",
    "    # Token级损失\n",
    "    per_token_loss = -per_token_ratio * adv_expanded\n",
    "    \n",
    "    # 掩码求和\n",
    "    loss = (per_token_loss * mask).sum() / mask.sum()\n",
    "    return loss\n",
    "\n",
    "# 模拟\n",
    "B, T = 2, 5\n",
    "per_token_ratio = torch.ones(B, T)  # [2, 5]\n",
    "advantages = torch.tensor([0.5, -0.5])  # [2]\n",
    "mask = torch.ones(B, T)\n",
    "\n",
    "loss = token_level_loss(per_token_ratio, advantages, mask)\n",
    "print(f\"Token级损失: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Overlong Reward Shaping\n",
    "\n",
    "### 公式\n",
    "$$R_{shaped} = R_{original} - \\lambda \\cdot \\max(0, |y| - L_{max})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlong_shaping(rewards, lengths, max_len, penalty=0.01):\n",
    "    \"\"\"过长奖励塑造\"\"\"\n",
    "    excess = torch.clamp(lengths - max_len, min=0)\n",
    "    return rewards - penalty * excess\n",
    "\n",
    "# 演示\n",
    "rewards = torch.tensor([1.0, 1.0, 1.0])\n",
    "lengths = torch.tensor([100, 500, 600])  # max=500\n",
    "\n",
    "shaped = overlong_shaping(rewards, lengths, max_len=500)\n",
    "print(f\"原始奖励: {rewards.tolist()}\")\n",
    "print(f\"回复长度: {lengths.tolist()}\")\n",
    "print(f\"塑造后:   {shaped.tolist()}\")\n",
    "print(\"\\n结果: 长度600超过500，被惩罚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 总结\n",
    "\n",
    "| 技术 | 解决的问题 | 核心改进 |\n",
    "|------|------------|----------|\n",
    "| Clip-Higher | 熵坍缩 | 解耦裁剪，$\\epsilon_{high} > \\epsilon_{low}$ |\n",
    "| Dynamic Sampling | 无效更新 | 过滤全零prompts |\n",
    "| Token-Level Loss | 粗粒度 | 逐token计算损失 |\n",
    "| Overlong Shaping | 截断问题 | 软惩罚替代硬截断 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
