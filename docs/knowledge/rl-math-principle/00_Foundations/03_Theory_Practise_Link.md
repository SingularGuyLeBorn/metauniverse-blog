# 理论与实践的桥梁：从测度到代码 (Theory to Practice)

## 1. 为什么代码里见不到测度论？

你在 `02_Implementation.py` 中看到的只有 `np.random.normal` 和 `mean()`，并没有显式的 $\sigma$-代数或勒贝格积分。这是为什么？

### 隐形的概率空间
当我们调用 `np.random.normal()` 时，计算机实际上是在模拟从概率空间 $(\Omega, \mathcal{F}, P)$ 中采样的过程。
- **$\Omega$**: 计算机伪随机数生成器的所有可能内部状态。
- **$P$**: 被设计为符合正态分布的频率特性。

### 大数定律 (LLN)
我们在代码中计算的 `mean_reward`：
```python
mean_reward = np.mean(rewards)
```
这背后的数学保证正是**大数定律**。只有当随机变量满足特定测度论条件（如独立同分布 i.i.d.，且期望存在）时，样本均值才会依概率收敛于理论期望（勒贝格积分）。

## 2. 连续性：从 GridWorld 到 MuJoCo

### 离散世界 (GridWorld)
- 理论：求和 $\sum$
- 代码：表格查找 (Q-Table)
- 不需要深厚的测度论，初等概率论足矣。

### 连续世界 (Robot Arm)
- 理论：积分 $\int$
- 代码：神经网络逼近 (Function Approximation)
- **关键点**：在连续空间中，单个点 $s$ 的概率通常为 0 ($P(S=s)=0$)。我们不能再说 "如果出现状态 $s$"，而必须说 "如果状态落入集合 $B$ 中"。这也是为什么我们需要 $\sigma$-代数——它定义了哪些集合 $B$ 是合法的"事件"。

## 3. 策略梯度定理的数学底座

在后续章节（如 PPO/GRPO）中，我们将推导策略梯度：
$$ \nabla J(\theta) = \int \nabla \pi_\theta(a|s) Q(s,a) da $$
这个公式成立的前提是积分和微分可以交换顺序（Leibniz Integral Rule）。这要求概率密度函数 $\pi_\theta$ 是光滑的，且满足一定的**可积性条件**。如果不理解测度论，就无法理解为什么某些"尖锐"的策略（如 Deterministic Policy）会导致梯度消失或爆炸。

## 4. 总结

- **理论**提供**保证**：告诉我们算法 *最终* 会收敛。
- **代码**提供**近似**：用有限的样本去逼近理论值。
- **测度论**是**裁判**：定义了什么样的数据是合法的，什么样的近似是有效的。
