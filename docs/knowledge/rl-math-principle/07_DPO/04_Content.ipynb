{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO算法：理论与代码逐块对应\n",
    "\n",
    "本Notebook将DPO (Direct Preference Optimization) 的核心公式与代码实现逐块对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 隐式奖励\n",
    "\n",
    "### 公式\n",
    "$$\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}$$\n",
    "\n",
    "### 解释\n",
    "DPO的核心洞察：语言模型本身可以作为隐式奖励模型。奖励由策略与参考模型的对数概率比给出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_implicit_reward(policy_logps, ref_logps, beta=0.1):\n",
    "    \"\"\"\n",
    "    计算隐式奖励\n",
    "    \n",
    "    r̂(x,y) = β * log(π_θ(y|x) / π_ref(y|x))\n",
    "           = β * (log π_θ(y|x) - log π_ref(y|x))\n",
    "    \"\"\"\n",
    "    return beta * (policy_logps - ref_logps)\n",
    "\n",
    "# 示例\n",
    "policy_logp = torch.tensor(-10.0)  # log π_θ\n",
    "ref_logp = torch.tensor(-12.0)     # log π_ref\n",
    "beta = 0.1\n",
    "\n",
    "reward = compute_implicit_reward(policy_logp, ref_logp, beta)\n",
    "print(f\"策略log概率: {policy_logp.item():.2f}\")\n",
    "print(f\"参考log概率: {ref_logp.item():.2f}\")\n",
    "print(f\"隐式奖励: {reward.item():.4f}\")\n",
    "print(f\"\\n解读: 策略给这个response更高的概率 → 正奖励\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DPO损失函数\n",
    "\n",
    "### 公式\n",
    "$$\\mathcal{L}_{DPO} = -\\mathbb{E}\\left[\\log\\sigma\\left(\\hat{r}(y_w) - \\hat{r}(y_l)\\right)\\right]$$\n",
    "\n",
    "展开：\n",
    "$$= -\\mathbb{E}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w)}{\\pi_{ref}(y_w)} - \\beta\\log\\frac{\\pi_\\theta(y_l)}{\\pi_{ref}(y_l)}\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
    "             ref_chosen_logps, ref_rejected_logps, beta=0.1):\n",
    "    \"\"\"\n",
    "    DPO损失函数\n",
    "    \n",
    "    L = -log σ(β * (log π_θ(y_w)/π_ref(y_w) - log π_θ(y_l)/π_ref(y_l)))\n",
    "    \"\"\"\n",
    "    # 步骤1: 计算log ratio\n",
    "    chosen_logratio = policy_chosen_logps - ref_chosen_logps\n",
    "    rejected_logratio = policy_rejected_logps - ref_rejected_logps\n",
    "    \n",
    "    # 步骤2: 计算logits\n",
    "    logits = beta * (chosen_logratio - rejected_logratio)\n",
    "    \n",
    "    # 步骤3: 二元交叉熵\n",
    "    loss = -F.logsigmoid(logits)\n",
    "    \n",
    "    return loss.mean(), logits\n",
    "\n",
    "# 示例：模型正确偏好chosen\n",
    "policy_chosen = torch.tensor(-8.0)    # 策略给chosen更高概率\n",
    "policy_rejected = torch.tensor(-10.0)\n",
    "ref_chosen = torch.tensor(-10.0)\n",
    "ref_rejected = torch.tensor(-10.0)\n",
    "\n",
    "loss, logits = dpo_loss(policy_chosen, policy_rejected, ref_chosen, ref_rejected, 0.1)\n",
    "print(f\"DPO损失: {loss.item():.4f}\")\n",
    "print(f\"Logits: {logits.item():.4f}\")\n",
    "print(f\"\\n解读: logits > 0 意味着模型正确地偏好chosen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 序列对数概率计算\n",
    "\n",
    "### 公式\n",
    "$$\\log\\pi_\\theta(y|x) = \\sum_{t=1}^{|y|} \\log P_\\theta(y_t | x, y_{<t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sequence_log_prob(logits, labels):\n",
    "    \"\"\"\n",
    "    从语言模型logits计算序列log概率\n",
    "    \n",
    "    log π(y|x) = Σ_t log P(y_t | y_{<t})\n",
    "    \"\"\"\n",
    "    # 对齐：logits[t] 预测 labels[t+1]\n",
    "    shift_logits = logits[:, :-1, :]  # [B, T-1, V]\n",
    "    shift_labels = labels[:, 1:]       # [B, T-1]\n",
    "    \n",
    "    # 每个位置的log概率\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "    \n",
    "    # 提取目标token的log概率\n",
    "    per_token_logp = torch.gather(\n",
    "        log_probs, dim=2, index=shift_labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    \n",
    "    # 序列总log概率\n",
    "    return per_token_logp.sum(dim=-1)\n",
    "\n",
    "# 模拟\n",
    "B, T, V = 2, 5, 100  # batch, seq_len, vocab\n",
    "logits = torch.randn(B, T, V)\n",
    "labels = torch.randint(0, V, (B, T))\n",
    "\n",
    "seq_logps = compute_sequence_log_prob(logits, labels)\n",
    "print(f\"序列log概率: {seq_logps.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. DPO变体\n",
    "\n",
    "### IPO (Identity Preference Optimization)\n",
    "$$\\mathcal{L}_{IPO} = \\left(\\hat{r}(y_w) - \\hat{r}(y_l) - \\frac{1}{\\beta}\\right)^2$$\n",
    "\n",
    "### SimPO (无参考模型)\n",
    "$$\\mathcal{L}_{SimPO} = -\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi(y_w) - \\frac{\\beta}{|y_l|}\\log\\pi(y_l) - \\gamma\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipo_loss(logits, beta=0.1):\n",
    "    \"\"\"IPO: 正则化DPO\"\"\"\n",
    "    target = 1.0 / beta\n",
    "    return (logits - target) ** 2\n",
    "\n",
    "def simpo_loss(policy_chosen_logps, policy_rejected_logps, \n",
    "               chosen_len, rejected_len, beta=2.0, gamma=0.5):\n",
    "    \"\"\"SimPO: 无参考模型 + 长度归一化\"\"\"\n",
    "    chosen_per_token = policy_chosen_logps / chosen_len\n",
    "    rejected_per_token = policy_rejected_logps / rejected_len\n",
    "    logits = beta * (chosen_per_token - rejected_per_token) - gamma\n",
    "    return -F.logsigmoid(logits)\n",
    "\n",
    "print(\"DPO变体对比:\")\n",
    "print(\"- DPO: 需要参考模型, log ratio\")\n",
    "print(\"- IPO: 添加正则化, 防止过拟合\")\n",
    "print(\"- SimPO: 无参考模型, 长度归一化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 总结\n",
    "\n",
    "| 组件 | 公式 | 代码 |\n",
    "|------|------|------|\n",
    "| 隐式奖励 | $\\hat{r} = \\beta\\log\\frac{\\pi}{\\pi_{ref}}$ | `beta * (policy_logps - ref_logps)` |\n",
    "| DPO损失 | $-\\log\\sigma(\\hat{r}_w - \\hat{r}_l)$ | `-F.logsigmoid(logits)` |\n",
    "| 序列log概率 | $\\sum_t \\log P(y_t\\|y_{<t})$ | `gather + sum` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
