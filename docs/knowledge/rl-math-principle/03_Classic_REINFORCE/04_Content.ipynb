{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE算法：理论与代码的逐块对应\n",
    "\n",
    "本Notebook将REINFORCE算法的理论公式与代码实现逐块对应，帮助理解每一行代码背后的数学原理。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入依赖\n",
    "\n",
    "首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子以保证可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 策略网络 $\\pi_\\theta(a|s)$\n",
    "\n",
    "### 理论公式\n",
    "\n",
    "策略是一个从状态到动作概率分布的映射。对于离散动作，我们使用Softmax函数：\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\frac{\\exp(h_\\theta(s, a))}{\\sum_{a'} \\exp(h_\\theta(s, a'))}$$\n",
    "\n",
    "其中：\n",
    "- $h_\\theta(s, a)$ 是神经网络对状态-动作对的评分（logits）\n",
    "- $\\theta$ 是神经网络的所有参数\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    策略网络：将状态映射到动作概率分布\n",
    "    \n",
    "    结构：\n",
    "    输入 s → 全连接层 → ReLU → 全连接层 → Softmax → 概率 π(a|s)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        # 第一层：状态 → 隐藏层\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        # 第二层：隐藏层 → 动作logits\n",
    "        self.fc2 = nn.Linear(hidden_size, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        数学过程：\n",
    "        1. h₁ = ReLU(W₁ · s + b₁)     -- 第一层\n",
    "        2. logits = W₂ · h₁ + b₂       -- 第二层（未归一化）\n",
    "        3. π(a|s) = Softmax(logits)    -- 归一化为概率\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))        # h₁ = ReLU(W₁ · s + b₁)\n",
    "        logits = self.fc2(x)               # h_θ(s, a)\n",
    "        action_probs = F.softmax(logits, dim=-1)  # Softmax归一化\n",
    "        return action_probs\n",
    "\n",
    "# 测试：创建一个策略网络\n",
    "state_dim = 4  # 例如CartPole的状态维度\n",
    "action_dim = 2  # 两个动作：左/右\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "\n",
    "# 输入一个随机状态\n",
    "test_state = torch.randn(1, 4)\n",
    "action_probs = policy(test_state)\n",
    "print(f\"状态: {test_state.numpy().flatten()}\")\n",
    "print(f\"动作概率: π(a=0|s)={action_probs[0,0]:.4f}, π(a=1|s)={action_probs[0,1]:.4f}\")\n",
    "print(f\"概率和: {action_probs.sum().item():.6f}（应该等于1）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 动作采样与对数概率\n",
    "\n",
    "### 理论公式\n",
    "\n",
    "根据策略分布采样动作：\n",
    "$$a \\sim \\pi_\\theta(\\cdot|s)$$\n",
    "\n",
    "同时计算对数概率（用于后续梯度计算）：\n",
    "$$\\log \\pi_\\theta(a|s)$$\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(policy, state):\n",
    "    \"\"\"\n",
    "    根据策略采样动作\n",
    "    \n",
    "    对应理论：\n",
    "    - a ~ π_θ(·|s)：从分布中采样\n",
    "    - 返回 log π_θ(a|s) 用于梯度计算\n",
    "    \"\"\"\n",
    "    # 转换为Tensor\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    \n",
    "    # 获取动作概率分布\n",
    "    action_probs = policy(state_tensor)\n",
    "    \n",
    "    # 创建分类分布（Categorical Distribution）\n",
    "    # 这是PyTorch对离散概率分布的封装\n",
    "    dist = Categorical(action_probs)\n",
    "    \n",
    "    # 从分布中采样动作：a ~ π_θ(·|s)\n",
    "    action = dist.sample()\n",
    "    \n",
    "    # 计算对数概率：log π_θ(a|s)\n",
    "    # 这就是策略梯度中的 ∇log π 的\"log π\"部分\n",
    "    log_prob = dist.log_prob(action)\n",
    "    \n",
    "    return action.item(), log_prob\n",
    "\n",
    "# 测试\n",
    "test_state = np.array([0.1, -0.2, 0.05, 0.3])\n",
    "action, log_prob = select_action(policy, test_state)\n",
    "print(f\"采样的动作: {action}\")\n",
    "print(f\"对数概率 log π(a|s): {log_prob.item():.4f}\")\n",
    "print(f\"对应的概率 π(a|s): {np.exp(log_prob.item()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 回报计算 $G_t$\n",
    "\n",
    "### 理论公式\n",
    "\n",
    "回报（Return）是从时刻 $t$ 开始的折扣奖励和：\n",
    "\n",
    "$$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\ldots = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k+1}$$\n",
    "\n",
    "使用递归公式可以高效计算：\n",
    "$$G_t = r_{t+1} + \\gamma G_{t+1}$$\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma):\n",
    "    \"\"\"\n",
    "    计算每个时刻的折扣回报\n",
    "    \n",
    "    使用递归公式：G_t = r_{t+1} + γ · G_{t+1}\n",
    "    从后往前计算，因为 G_t 依赖于 G_{t+1}\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    returns = [0.0] * T\n",
    "    \n",
    "    # 边界条件：最后一步\n",
    "    # G_{T-1} = r_T（没有更多未来奖励）\n",
    "    returns[T-1] = rewards[T-1]\n",
    "    \n",
    "    # 从后往前递归\n",
    "    for t in range(T-2, -1, -1):\n",
    "        # G_t = r_{t+1} + γ · G_{t+1}\n",
    "        returns[t] = rewards[t] + gamma * returns[t+1]\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# 示例：一个5步的轨迹\n",
    "rewards = [1, 1, 1, 1, 1]  # 每步奖励都是1\n",
    "gamma = 0.99\n",
    "returns = compute_returns(rewards, gamma)\n",
    "\n",
    "print(\"时刻 t | 奖励 r | 回报 G_t\")\n",
    "print(\"-\" * 30)\n",
    "for t, (r, g) in enumerate(zip(rewards, returns)):\n",
    "    print(f\"  {t}    |   {r}   | {g:.4f}\")\n",
    "\n",
    "print(f\"\\n验证：G_0 应该 = 1 + 0.99×1 + 0.99²×1 + 0.99³×1 + 0.99⁴×1\")\n",
    "theoretical = sum([gamma**k for k in range(5)])\n",
    "print(f\"理论值: {theoretical:.4f}, 计算值: {returns[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 回报标准化\n",
    "\n",
    "### 为什么需要标准化？\n",
    "\n",
    "问题：如果所有回报 $G_t$ 都是正数，那么所有动作的概率都会增加。\n",
    "\n",
    "解决方案：将回报标准化为均值0、标准差1的分布：\n",
    "$$G'_t = \\frac{G_t - \\mu_G}{\\sigma_G + \\epsilon}$$\n",
    "\n",
    "这样，约一半的 $G'_t$ 是正的（增大概率），一半是负的（减小概率）。\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_returns(returns):\n",
    "    \"\"\"\n",
    "    标准化回报\n",
    "    \n",
    "    公式：G'_t = (G_t - mean) / (std + ε)\n",
    "    \"\"\"\n",
    "    returns_tensor = torch.FloatTensor(returns)\n",
    "    mean = returns_tensor.mean()\n",
    "    std = returns_tensor.std()\n",
    "    eps = 1e-8  # 防止除零\n",
    "    normalized = (returns_tensor - mean) / (std + eps)\n",
    "    return normalized\n",
    "\n",
    "# 测试\n",
    "raw_returns = [10, 8, 6, 12, 9]\n",
    "normalized = normalize_returns(raw_returns)\n",
    "\n",
    "print(\"原始回报:\", raw_returns)\n",
    "print(f\"均值: {np.mean(raw_returns):.2f}, 标准差: {np.std(raw_returns):.2f}\")\n",
    "print(\"\\n标准化后:\")\n",
    "for t, (g, g_norm) in enumerate(zip(raw_returns, normalized.tolist())):\n",
    "    sign = \"+\" if g_norm > 0 else \"\"\n",
    "    print(f\"  G_{t}={g} → G'_{t}={sign}{g_norm:.3f}\")\n",
    "print(f\"\\n标准化后均值: {normalized.mean():.6f}（应接近0）\")\n",
    "print(f\"标准化后标准差: {normalized.std():.6f}（应接近1）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 策略梯度损失\n",
    "\n",
    "### 理论公式\n",
    "\n",
    "策略梯度定理给出了目标函数的梯度：\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "在PyTorch中，我们定义**损失函数**（因为PyTorch做梯度下降来最小化损失）：\n",
    "$$\\text{Loss} = -\\sum_t \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$$\n",
    "\n",
    "**注意负号**：因为 $\\min(-J) = \\max(J)$\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_loss(log_probs, returns):\n",
    "    \"\"\"\n",
    "    计算策略梯度损失\n",
    "    \n",
    "    Loss = -Σ_t log π(a_t|s_t) · G_t\n",
    "    \n",
    "    负号的原因：\n",
    "    - 我们想最大化期望回报 J(θ)\n",
    "    - PyTorch的optimizer.step()做的是梯度下降（最小化）\n",
    "    - 所以 loss = -J(θ)，最小化 -J 等于最大化 J\n",
    "    \"\"\"\n",
    "    # 将log_prob列表转换为张量\n",
    "    log_probs_tensor = torch.stack(log_probs)\n",
    "    \n",
    "    # 元素乘法：每个 log_prob 乘以对应的 return\n",
    "    weighted_log_probs = log_probs_tensor * returns\n",
    "    \n",
    "    # 求和并取负\n",
    "    loss = -weighted_log_probs.sum()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 模拟演示\n",
    "# 假设我们采集了一个3步的轨迹\n",
    "fake_log_probs = [torch.tensor(-0.5), torch.tensor(-0.3), torch.tensor(-0.8)]\n",
    "fake_returns = torch.tensor([1.5, 0.2, -0.7])  # 标准化后的回报\n",
    "\n",
    "loss = compute_policy_loss(fake_log_probs, fake_returns)\n",
    "print(f\"策略损失: {loss.item():.4f}\")\n",
    "print(\"\\n分解：\")\n",
    "for t, (lp, g) in enumerate(zip(fake_log_probs, fake_returns)):\n",
    "    contribution = -(lp.item() * g.item())\n",
    "    print(f\"  步骤{t}: -({lp.item():.2f} × {g.item():.2f}) = {contribution:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 完整的REINFORCE更新\n",
    "\n",
    "### 算法流程\n",
    "\n",
    "1. 使用当前策略 $\\pi_\\theta$ 采集一个完整回合\n",
    "2. 计算每步的回报 $G_t$\n",
    "3. 标准化回报\n",
    "4. 计算损失 $L = -\\sum_t \\log\\pi(a_t|s_t) \\cdot G_t$\n",
    "5. 反向传播计算梯度\n",
    "6. 更新参数 $\\theta \\leftarrow \\theta - \\alpha \\nabla L$（注意是减，因为loss是负的J）\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_update(policy, optimizer, log_probs, rewards, gamma):\n",
    "    \"\"\"\n",
    "    REINFORCE算法的一次完整更新\n",
    "    \n",
    "    对应算法伪代码：\n",
    "    1. 计算回报 G_t = Σ γ^k r_{t+k+1}\n",
    "    2. 标准化回报\n",
    "    3. 计算损失 Loss = -Σ log π · G\n",
    "    4. θ ← θ + α · ∇_θ J（通过 optimizer 实现）\n",
    "    \"\"\"\n",
    "    # 步骤1：计算回报\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    \n",
    "    # 步骤2：标准化\n",
    "    returns_normalized = normalize_returns(returns)\n",
    "    \n",
    "    # 步骤3：计算损失\n",
    "    loss = compute_policy_loss(log_probs, returns_normalized)\n",
    "    \n",
    "    # 步骤4：梯度更新\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    loss.backward()        # 反向传播计算 ∇_θ Loss\n",
    "    optimizer.step()       # θ ← θ - α · ∇_θ Loss（等价于 θ ← θ + α · ∇_θ J）\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"REINFORCE更新函数已定义！\")\n",
    "print(\"\\n完整的算法流程：\")\n",
    "print(\"1. 采集轨迹 τ = (s₀,a₀,r₁,s₁,a₁,r₂,...)\")\n",
    "print(\"2. 计算 G_t = r_{t+1} + γG_{t+1}\")\n",
    "print(\"3. 标准化 G'_t = (G_t - μ) / σ\")\n",
    "print(\"4. Loss = -Σ log π(a_t|s_t) · G'_t\")\n",
    "print(\"5. θ ← θ - α · ∇Loss = θ + α · ∇J\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 总结\n",
    "\n",
    "| 理论概念 | 数学公式 | 代码实现 |\n",
    "|----------|----------|----------|\n",
    "| 策略 | $\\pi_\\theta(a|s) = \\text{Softmax}(h_\\theta)$ | `PolicyNetwork.forward()` |\n",
    "| 采样 | $a \\sim \\pi_\\theta(\\cdot|s)$ | `Categorical.sample()` |\n",
    "| 对数概率 | $\\log \\pi_\\theta(a|s)$ | `Categorical.log_prob()` |\n",
    "| 回报 | $G_t = r_{t+1} + \\gamma G_{t+1}$ | `compute_returns()` |\n",
    "| 标准化 | $G'_t = (G_t - \\mu) / \\sigma$ | `normalize_returns()` |\n",
    "| 损失 | $L = -\\sum \\log\\pi \\cdot G$ | `compute_policy_loss()` |\n",
    "| 更新 | $\\theta \\leftarrow \\theta + \\alpha \\nabla J$ | `optimizer.step()` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
