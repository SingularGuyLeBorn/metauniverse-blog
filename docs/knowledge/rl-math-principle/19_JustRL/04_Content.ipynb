{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JustRL vs PPO: 极简主义大对决\n",
    "\n",
    "本笔记本展示了 JustRL (2512.16649) 相比标准 PPO 的简洁性。\n",
    "我们将实现两者的虚拟 Loss 函数，并在概念层面对比它们的复杂度与显存占用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 模拟数据\n",
    "B, G, Vocab = 2, 4, 100\n",
    "log_probs = torch.randn(B, G, requires_grad=True)\n",
    "old_log_probs = log_probs.detach() + torch.randn(B, G) * 0.1\n",
    "rewards = torch.randn(B, G)\n",
    "values = torch.randn(B, G) # 仅 PPO 需要\n",
    "ref_log_probs = log_probs.detach() + 0.05 # 仅 PPO 需要\n",
    "\n",
    "print(\"数据初始化完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PPO Loss (繁杂之路)\n",
    "\n",
    "标准 PPO 需要：\n",
    "- 重要性采样比率 (Importance Sampling Ratio)\n",
    "- 截断 (Clipping)\n",
    "- 价值损失 (Value Loss)\n",
    "- KL 散度惩罚\n",
    "- 熵奖励 (Entropy Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss_demo():\n",
    "    # 1. 优势函数 (通常是 GAE, 这里简化为 R-V)\n",
    "    adv = rewards - values # 需要 Critic\n",
    "    \n",
    "    # 2. 比率\n",
    "    ratio = torch.exp(log_probs - old_log_probs)\n",
    "    \n",
    "    # 3. 截断 (Clip)\n",
    "    surr1 = ratio * adv\n",
    "    surr2 = torch.clamp(ratio, 0.8, 1.2) * adv\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    # 4. 价值损失 (需要训练 Critic)\n",
    "    value_loss = F.mse_loss(values, rewards)\n",
    "    \n",
    "    # 5. KL 惩罚\n",
    "    kl = (log_probs - ref_log_probs).mean()\n",
    "    \n",
    "    # 总损失\n",
    "    total_loss = policy_loss + 0.5 * value_loss + 0.1 * kl\n",
    "    return total_loss\n",
    "\n",
    "print(f\"PPO 损失计算完毕: {ppo_loss_demo().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. JustRL Loss (极简配方)\n",
    "\n",
    "JustRL 只需要：\n",
    "- 组归一化 (Group Normalization)\n",
    "- 策略梯度 (Policy Gradient)\n",
    "- ... 没了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def justrl_loss_demo():\n",
    "    # 1. 组归一化优势计算\n",
    "    mean = rewards.mean(dim=1, keepdim=True)\n",
    "    std = rewards.std(dim=1, keepdim=True) + 1e-8\n",
    "    adv = (rewards - mean) / std\n",
    "    \n",
    "    # 2. 策略梯度 (Vanilla)\n",
    "    loss = -(adv * log_probs).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(f\"JustRL 损失计算完毕: {justrl_loss_demo().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 对比总结\n",
    "\n",
    "注意 JustRL 的逻辑代码中完全没有 `torch.clamp`, `value_loss`, 和 `ref_log_probs`。\n",
    "这意味着在前向传播时，我们**不需要加载** 参考模型 (Ref Model) 或 评论家模型 (Critic Model) 到 GPU 显存中。\n",
    "\n",
    "**判决**: JustRL 就是 \"Just\" RL (仅仅是 RL，没别的)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
