{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO算法：理论与代码逐块对应\n",
    "\n",
    "本Notebook将GRPO (Group Relative Policy Optimization) 的核心公式与代码实现逐块对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 组优势估计\n",
    "\n",
    "### 公式\n",
    "$$A_i = \\frac{R_i - \\bar{R}}{\\sigma_R + \\epsilon}  \\quad \\text{(原始GRPO)}$$\n",
    "$$A_i = R_i - \\bar{R}  \\quad \\text{(Dr. GRPO, 推荐)}$$\n",
    "\n",
    "其中: $\\bar{R} = \\frac{1}{G}\\sum_{j=1}^G R_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_advantages(rewards, group_size, use_std_norm=False, eps=1e-5):\n",
    "    \"\"\"\n",
    "    计算GRPO组优势\n",
    "    \n",
    "    A_i = R_i - mean(R)  [Dr. GRPO]\n",
    "    A_i = (R_i - mean(R)) / std(R)  [原始GRPO]\n",
    "    \"\"\"\n",
    "    num_prompts = len(rewards) // group_size\n",
    "    \n",
    "    # 重塑为 [num_prompts, group_size]\n",
    "    rewards = rewards.view(num_prompts, group_size)\n",
    "    \n",
    "    # 组内均值\n",
    "    mean_r = rewards.mean(dim=1, keepdim=True)\n",
    "    advantages = rewards - mean_r\n",
    "    \n",
    "    # 可选归一化\n",
    "    if use_std_norm:\n",
    "        std_r = rewards.std(dim=1, keepdim=True)\n",
    "        advantages = advantages / (std_r + eps)\n",
    "    \n",
    "    return advantages.view(-1)\n",
    "\n",
    "# 示例: 2个prompt, 每个4个response\n",
    "rewards = torch.tensor([\n",
    "    1.0, 0.0, 0.5, 0.5,  # Prompt 1: mean=0.5\n",
    "    0.0, 0.0, 1.0, 1.0   # Prompt 2: mean=0.5\n",
    "])\n",
    "\n",
    "advantages = compute_grpo_advantages(rewards, group_size=4)\n",
    "print(f\"奖励: {rewards.tolist()}\")\n",
    "print(f\"优势: {advantages.tolist()}\")\n",
    "print(f\"\\n解读: 高于均值→正优势, 低于均值→负优势\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. GRPO损失函数\n",
    "\n",
    "### 公式 (与PPO相同结构)\n",
    "$$L^{GRPO} = -\\mathbb{E}\\left[\\min(r(\\theta) A, \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon) A)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_loss(new_log_probs, old_log_probs, advantages, clip_epsilon=0.2):\n",
    "    \"\"\"\n",
    "    GRPO损失\n",
    "    \n",
    "    L = -E[min(r·A, clip(r)·A)]\n",
    "    \"\"\"\n",
    "    # 概率比\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # 裁剪\n",
    "    clipped = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "    \n",
    "    # 取较小值\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = clipped * advantages\n",
    "    loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    return loss, ratio\n",
    "\n",
    "# 模拟\n",
    "new_logp = torch.tensor([-10.0, -12.0, -11.0, -11.5])  # 新策略\n",
    "old_logp = torch.tensor([-10.5, -11.5, -11.5, -11.5])  # 旧策略\n",
    "advs = torch.tensor([0.5, -0.5, 0.0, 0.0])              # 优势\n",
    "\n",
    "loss, ratio = grpo_loss(new_logp, old_logp, advs)\n",
    "print(f\"GRPO损失: {loss.item():.4f}\")\n",
    "print(f\"概率比: {ratio.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dr. GRPO vs 原始GRPO\n",
    "\n",
    "### 问题: 标准差归一化可能导致信号放大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试: 几乎相同的奖励\n",
    "rewards_similar = torch.tensor([1.0, 1.0, 1.0, 1.01])\n",
    "\n",
    "adv_original = compute_grpo_advantages(rewards_similar, 4, use_std_norm=True)\n",
    "adv_dr = compute_grpo_advantages(rewards_similar, 4, use_std_norm=False)\n",
    "\n",
    "print(\"测试: 几乎相同的奖励\")\n",
    "print(f\"奖励: {rewards_similar.tolist()}\")\n",
    "print(f\"原始GRPO优势: {adv_original.tolist()}\")\n",
    "print(f\"Dr. GRPO优势: {adv_dr.tolist()}\")\n",
    "print(f\"\\n问题: 原始GRPO放大了微小差异 ({adv_original[-1]:.2f})!\")\n",
    "print(f\"推荐: 使用Dr. GRPO (不归一化)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GRPO vs PPO对比\n",
    "\n",
    "### 核心区别: 优势计算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GRPO vs PPO优势计算对比\")\n",
    "print(\"=\"*50)\n",
    "print(\"\")\n",
    "print(\"PPO:\")\n",
    "print(\"  A_t = r_t + γV(s_{t+1}) - V(s_t)  (需要价值网络)\")\n",
    "print(\"\")\n",
    "print(\"GRPO:\")\n",
    "print(\"  A_i = R_i - mean(R_1, ..., R_G)   (使用组均值)\")\n",
    "print(\"\")\n",
    "print(\"GRPO优势:\")\n",
    "print(\"  ✓ 无需额外的价值网络\")\n",
    "print(\"  ✓ 节省显存和计算\")\n",
    "print(\"  ✓ 更适合LLM生成任务\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 总结\n",
    "\n",
    "| 组件 | 公式 | 代码 |\n",
    "|------|------|------|\n",
    "| 组优势 | $A_i = R_i - \\bar{R}$ | `rewards - rewards.mean(dim=1)` |\n",
    "| 概率比 | $r = \\pi/\\pi_{old}$ | `exp(new_logp - old_logp)` |\n",
    "| 损失 | $-\\min(rA, clip(r)A)$ | `-torch.min(surr1, surr2)` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
