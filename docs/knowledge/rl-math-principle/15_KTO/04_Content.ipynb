{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTO 损失函数可视化：数学中的心理学\n",
    "\n",
    "本笔记本聚焦于 **KTO 损失函数**，它源自 Kahneman & Tversky 的前景理论 (Prospect Theory)。\n",
    "我们将可视化损失函数（以及梯度）在处理“被选中”(Chosen/Gain) 和 “被拒绝”(Rejected/Loss) 样本时的不同行为，模拟“**损失厌恶 (Loss Aversion)**”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 超参数\n",
    "beta = 0.5\n",
    "lambda_chosen = 1.0\n",
    "lambda_rejected = 1.33 # 损失厌恶系数：我们对失败的痛恨大于对成功的喜悦\n",
    "KL_est = 0.0 # 为了方便观察，假设 Z(x) ~ KL 为 0\n",
    "\n",
    "# 隐式奖励范围 (Log Ratio)\n",
    "r_vals = torch.linspace(-5, 5, 100)\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 价值函数形状 (The Value Function Shape)\n",
    "\n",
    "KTO 分别处理 Chosen 和 Rejected 数据点。\n",
    "- **Chosen**: 我们希望最大化 `L_chosen(r)`\n",
    "- **Rejected**: 我们希望最小化 `L_rejected(r)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kto_loss_chosen(r):\n",
    "    # 标准 logistic loss\n",
    "    return 1 - F.sigmoid(beta * (r - KL_est))\n",
    "\n",
    "def kto_loss_rejected(r):\n",
    "    # 注意这里的反转：我们希望 r 远小于 KL reference\n",
    "    # 以及标量权重 lambda_rejected\n",
    "    raw_loss = 1 - F.sigmoid(beta * (KL_est - r))\n",
    "    return lambda_rejected * raw_loss\n",
    "\n",
    "loss_c = kto_loss_chosen(r_vals)\n",
    "loss_r = kto_loss_rejected(r_vals)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(r_vals.numpy(), loss_c.numpy(), label='Loss (Chosen/选中)', color='green')\n",
    "plt.plot(r_vals.numpy(), loss_r.numpy(), label='Loss (Rejected/拒绝)', color='red')\n",
    "plt.title(f\"KTO 损失地形图 (Lambda_rej={lambda_rejected})\")\n",
    "plt.xlabel(\"隐式奖励 (Implicit Reward / Log Ratio)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 梯度分析 (The 'Push')\n",
    "\n",
    "注意 **红线 (Rejected)** 比 绿线 在相同误差幅度下更陡峭/更高吗？\n",
    "这意味着模型会感受到一股更强的“推力”去修正一个糟糕的错误（选中了本该拒绝的样本），相比之下，改善一个好的答案的动力则较小。\n",
    "\n",
    "这种**不对称性**防止了模型为了偶尔的高奖励而“胡言乱语”。它强制了安全性/保守性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
