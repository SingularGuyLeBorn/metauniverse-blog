{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORPO损失函数：交互式演示\n",
    "\n",
    "演示Odds Ratio相比于普通Probability在区分好坏样本时的梯度放大效应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Odds vs Probability\n",
    "看看当概率 $p$ 从 0.5 增加到 0.99 时，Odds 是如何变化的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.linspace(0.01, 0.99, 100)\n",
    "odds = probs / (1 - probs)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(probs, odds)\n",
    "plt.title(\"Odds vs Probability\")\n",
    "plt.xlabel(\"Probability p\")\n",
    "plt.ylabel(\"Odds p/(1-p)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(probs, torch.log(odds))\n",
    "plt.title(\"Log Odds vs Probability\")\n",
    "plt.xlabel(\"Probability p\")\n",
    "plt.ylabel(\"Log Odds\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 计算 ORPO Loss\n",
    "假设我们有一组 Chosen 和 Rejected 的 Log Probs。\n",
    "Chosen: -0.5 (p=0.60)\n",
    "Rejected: -2.0 (p=0.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_odds(log_p):\n",
    "    return log_p - torch.log1p(-torch.exp(log_p))\n",
    "\n",
    "log_p_w = torch.tensor([-0.5])\n",
    "log_p_l = torch.tensor([-2.0])\n",
    "\n",
    "lo_w = compute_log_odds(log_p_w)\n",
    "lo_l = compute_log_odds(log_p_l)\n",
    "\n",
    "print(f\"Log Odds Chosen ({log_p_w.item()}): {lo_w.item():.2f}\")\n",
    "print(f\"Log Odds Rejected ({log_p_l.item()}): {lo_l.item():.2f}\")\n",
    "\n",
    "diff = lo_w - lo_l\n",
    "loss = -F.logsigmoid(diff)\n",
    "print(f\"ORPO Loss contribution: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 梯度敏感度分析\n",
    "如果在 DPO 中，奖励是由 policy/ref 的比率决定的。\n",
    "在 ORPO 中，由于 Odds 函数在 p>0.5 时增长极快，模型会受到极强的信号去进一步推高 Chosen 的概率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
