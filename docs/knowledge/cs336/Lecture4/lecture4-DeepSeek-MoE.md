DeepSeek-v1 已经是接近两年前的模型了, 那时候 DeepSeek 团队就已经搞定了 MoE 架构

本篇就从 v1 都 v3 梳理他们从最早的尝试到大规模训练都有哪些改变

### DeepSeek MoE V1 (16B 总参数 - 2.8B 激活参数)
DeepSeek MoE V1 的核心思想是在标准的 Transformer 架构中, 将前馈网络 (Feed-Forward Network, FFN) 部分替换为一个专家混合 (Mixture of Experts, MoE)层. 这个 MoE 层包含两种类型的专家: **共享专家 (Shared Experts)**和**路由专家 (Routed Experts)**.

![](https://cdn.nlark.com/yuque/0/2025/png/42982692/1758964287470-2e671530-c289-4f2a-81b0-83f25c527147.png)

> 图 1: DeepSeek MoE V1 架构示意图.
>

如上图所示, 对于每个输入的 token, 它会同时被所有共享专家处理, 并由一个**路由器 (Router)**选择一部分路由专家进行处理. 这种设计的动机是让共享专家学习所有 token 都需要的通用知识, 而让路由专家学习更细粒度的, 特定领域的知识. V1 版本包含**2 个共享专家**和**64 个路由专家**, 路由器会从中为每个 token 选择**得分最高的 4 个**进行激活.

#### 标准 Top-k 路由机制 (Standard, top-k routing)
路由器的决策过程通过以下公式实现:

$$ h'_t = u_t + \sum_{i=1}^{N_s} \text{FFN}_i^{(s)}(u_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}_i^{(r)}(u_t) $$
$$ g_{i,t} =
\begin{cases}
s_{i,t}, & \text{if } s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \le j \le N_r\}, K_r) \\
0, & \text{otherwise}
\end{cases} $$
$$ s_{i,t} = \text{Softmax}_i(u_t^T e_i) $$

+ **变量解释**:
    - $ h'_t $: MoE 层的最终输出.
    - $ u_t $: MoE 层的输入 token 表示. 最终的输出 $ h'_t $ 通过残差连接加入了原始输入 $ u_t $.
    - $ \text{FFN}_i^{(s)} $: 第 $ i $ 个**共享专家**. 总共有 $ N_s $ 个 (V1中 $ N_s=2 $). 所有输入 $ u_t $ 都会经过所有共享专家的计算.
    - $ \text{FFN}_i^{(r)} $: 第 $ i $ 个**路由专家**. 总共有 $ N_r $ 个 (V1中 $ N_r=64 $).
    - $ s_{i,t} $: 路由器为输入 $ u_t $ 对第 $ i $ 个路由专家计算出的**路由分数 (或权重)**. 这是通过计算 $ u_t $ 与专家 $ i $ 的可训练嵌入向量 $ e_i $ 的点积, 再经过 Softmax 函数得到的. Softmax 确保所有专家的分数和为 1.
    - $ g_{i,t} $: 最终用于加权求和的**门控值 (gating value)**. `Topk` 函数会从所有 $ N_r $ 个专家的分数 $ \{s_{j,t}\} $ 中选出最高的 $ K_r $ 个 (V1中 $ K_r=4 $). 只有被选中的专家的 $ g_{i,t} $ 等于其原始分数 $ s_{i,t} $, 其他专家的 $ g_{i,t} $ 都为 0.
    - 这个过程确保了对于每个 token, 只有 $ K_r $ 个路由专家被激活, 实现了模型的稀疏性.

#### 标准辅助损失函数 (Standard Aux-loss balancing)
为了防止路由器总是将 token 发送给少数几个受欢迎的专家, 导致负载不均, 训练时引入了一个辅助损失函数来鼓励负载均衡.

$$ L_{\text{ExpBal}} = \alpha_1 \sum_{i=1}^{N_r} f_i P_i $$
$$ f_i = \frac{1}{K_r T} \sum_{t=1}^{T} \mathbb{1}(\text{Token } t \text{ selects Expert } i) $$
$$ P_i = \frac{1}{T} \sum_{t=1}^{T} s_{i,t} $$

+ **变量解释**:
    - $ L_{\text{ExpBal}} $: 专家平衡辅助损失.
    - $ \alpha_1 $: 一个超参数, 用于控制该损失项的权重.
    - $ T $: 一个 batch 中的 token 总数.
    - $ f_i $: 第 $ i $ 个专家在 batch 中被**选择的频率**. 它计算了路由到专家 $ i $ 的 token 数量占总路由数量的比例.
    - $ P_i $: 第 $ i $ 个专家在 batch 中获得的**平均路由分数**.
    - 通过最小化 $ L_{\text{ExpBal}} $ (即 $ f_i $ 和 $ P_i $ 的乘积), 模型被激励去让更多的专家被利用起来, 并且路由器的打分也趋向于更均匀.

### DeepSeek MoE V2 (236B 总参数 - 21B 激活参数)
V2 版本将模型规模大幅提升, 拥有 **2 个共享专家**和**160 个路由专家**, 每次激活**10 个**. 随着模型和专家数量的急剧增加, 训练的系统级挑战, 特别是设备间的通信开销, 变得至关重要. V2 的主要创新点在于优化路由策略以降低通信成本.

![](https://cdn.nlark.com/yuque/0/2025/png/42982692/1758964321375-9995a312-a0df-43e4-bca1-72520aca7f7b.png)

> 图 2: DeepSeek MoE V2 架构示意图.


相比 V1, V2 在架构上没有任何区别, 只是专家数量变化了 . 损失函数也是一样的, 但他们增加了一个非常巧妙的方法用于解决细粒度专家的缺点

> 当专家粒度过小, 数量很多, 那么就会有很多活跃专家, 每次输入都需要将 token 通过路由发送给不同设备上的大量专家, 这会导致巨大的通信开销


DeepSeek 团队采用了Top-M 设备路由 (Top-M device routing)去解决这个事情(果然是 Infra 的神！但我其实更期待 Google 团队的 Infra 经验分享)

#### Top-M 设备路由 (Top-M device routing)
当专家被分散到大量不同的计算设备 (如 GPU) 上时, 如果一个 token 需要的专家分布在很多设备上, 就会产生巨大的通信开销. Top-M 路由正是为了解决这个问题.

+ **核心思想**: 在为 token 选择 Top-K 专家之前,**首先选择 Top-M 个设备**. 具体来说, 路由器先计算出所有专家的分数, 然后将分数按设备进行汇总, 选出总分最高的 M 个设备. 之后, 路由选择被限制在这 M 个设备内部进行 Top-K 专家选择. 这样, 每个 token 最多只需要与 M 个设备通信, 极大地控制了通信带宽.

#### 通信平衡损失 (Communication balancing loss)
除了专家负载, 设备间的通信负载也需要平衡. V2 引入了一个新的损失项来平衡设备间 "输入" (token 发往设备) 和 "输出" (计算结果返回) 的通信量.

$$ L_{\text{CommBal}} = \alpha_2 \sum_{j=1}^{D} l_j \cdot r_j $$
$$ l_j = \frac{1}{MT} \sum_{t=1}^{T} \mathbb{1}(\text{Token } t \text{ is sent to Device } j) $$
$$ r_j = \frac{1}{T} \sum_{t=1}^{T} \sum_{i \in \text{Experts on Device } j} s_{i,t} $$

+ **变量解释**:
    - $ L_{\text{CommBal}} $: 通信平衡损失.
    - $ D $: 设备总数.
    - $ l_j $: 发送到设备 $ j $ 的**token 比例**, 代表了输入通信负载.
    - $ r_j $: 路由到设备 $ j $ 上所有专家的**总分数占比**, 代表了输出通信负载.
    - 这个损失函数鼓励模型将 token 和路由权重均匀地分布到不同的设备上.

### DeepSeek MoE V3 (671B 总参数 - 37B 激活参数)
V3 是目前规模最大的版本, 拥有 **1 个共享专家**和**258 个路由专家**, 每次激活**8 个**. V3 在路由算法和平衡策略上做了进一步的优化.

![](https://cdn.nlark.com/yuque/0/2025/png/42982692/1758964344957-36883d4d-c179-4252-8b28-25cba1c832a4.png)

> 图 3: DeepSeek MoE V3 架构示意图.

#### Sigmoid+Softmax TopK 路由
V3 调整了门控值的计算方式, 引入了 Sigmoid 函数, 并且只对被选中的专家进行归一化.

$$ h'_t = u_t + \sum_{i=1}^{N_s} \text{FFN}_i^{(s)}(u_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}_i^{(r)}(u_t) $$

$$ g_{i,t} = \frac{g'_{i,t}}{\sum_{j=1}^{N_r} g'_{j,t}} $$
$$ g'_{i,t} =
\begin{cases}
s_{i,t}, & \text{if } s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \le j \le N_r\}, K_r) \\
0, & \text{otherwise}
\end{cases} $$
$$ s_{i,t} = \text{Sigmoid}(u_t^T e_i) $$

+ **主要变化**:
    - 专家分数 $ s_{i,t} $ 由原始的点积得分经过 **Sigmoid** 函数得到, 而不是直接送入 Softmax. Sigmoid 函数将得分映射到 (0, 1) 区间, 相比 Softmax 行为更平滑, 数值更稳定.
    - 最终的门控值 $ g_{i,t} $ 通过对被选中的 Top-K 个专家的分数 $ g'_{i,t} $ 进行归一化 (相当于一个 Softmax) 得到. 这样保证了最终用于加权的系数之和为 1.

#### 无辅助损失的序列级均衡 (Aux-loss-free + seq-wise aux)
V3 抛弃了 V1/V2 中的显式辅助损失函数, 转而采用一种无损失的均衡策略. 此外, 它还考虑了在单个序列级别上进行均衡.

+ **无辅助损失**: 虽然图中公式未详细展开, 但其思想是通过为每个专家引入一个可动态调整的偏置项 (bias) $ b_i $ 来实现均衡. 当一个专家被过度使用时, 其偏置 $ b_i $ 会被调低, 从而降低其后续被选中的概率, 反之亦然. 这样可以在没有额外损失项的情况下实现负载均衡.
+ **序列级均衡**: V1/V2 的均衡策略是基于整个 batch 的. 但在推理时, 模型可能接收到一些分布外的序列, 导致某些专家在处理该序列时被集中调用. 序列级均衡旨在确保在单个序列的处理过程中, 专家的使用也是均衡的, 从而增强模型在推理时的鲁棒性.

### DeepSeek MoE V3 的注意力机制变体: MLA (Multi-Head Latent Attention)
除了 MoE 架构的改进, V3 还引入了一种新颖的注意力机制 MLA, 用于优化推理过程中的 KV 缓存大小, 这对于处理长序列至关重要.

![](https://cdn.nlark.com/yuque/0/2025/png/42982692/1758964414027-d37f3c4c-a79c-4f8c-906e-3b4a5abbf5e1.png)

> 图 4: Multi-Head Latent Attention (MLA) 架构图.


+ **核心思想**: 传统注意力机制需要缓存每个 token 的 Key (K) 和 Value (V) 向量, 维度较高. MLA 的做法是, 先将输入 $ h_t $ 投影到一个**维度更低的 "潜" (latent) 空间**, 得到 $ c_t^{KV} $, 然后只缓存这个低维的 $ c_t^{KV} $. 在实际计算注意力时, 再从 $ c_t^{KV} $ "上采样" 恢复出 K 和 V 向量.

$$ c_t^{KV} = W^{DKV} h_t $$
$$ k_t^c = W^{UK} c_t^{KV} $$
$$ v_t^c = W^{UV} c_t^{KV} $$

+ **变量解释**:
    - $ h_t $: 注意力层的输入.
    - $ c_t^{KV} $: 低维的潜激活向量, 这是实际被缓存的内容.
    - $ W^{DKV} $: 将 $ h_t $ 投影到潜空间的**降维矩阵**.
    - $ W^{UK}, W^{UV} $: 分别从 $ c_t^{KV} $ 恢复 K 和 V 的**升维矩阵**.
+ **优势**:
    - **节省缓存**: 由于 $ c_t^{KV} $ 的维度远小于原始的 K, V 向量, KV 缓存的内存占用被显著降低.
    - **计算效率**: 看似增加了额外的矩阵乘法, 但升维矩阵 $ W^{UK} $ 可以在计算中与查询 (Query) 的投影矩阵 $ W^Q $ 合并, 从而不增加额外的计算开销.
+ **与 RoPE 的冲突**: MLA 的矩阵合并技巧与旋转位置编码 (RoPE) 存在冲突, 因为 RoPE 的旋转操作会插入到两个可合并的矩阵之间. DeepSeek 的解决方案是保留少数几个维度不进行压缩, 专门用于执行旋转操作.

---

### 从 V1 到 V3 的演进总结
DeepSeek MoE 架构的演进清晰地展示了从一个基础可行的 MoE 模型到一个高度优化的, 能够支撑超大规模训练和推理的系统性解决方案的路径.

+ **Motivation 与改变**:
    - **从 V1 到 V2**: 核心动机是**解决超大规模训练带来的系统瓶颈**. 当专家数量从 64 增加到 160 时, 跨设备通信成为主要矛盾. 为此, V2 引入了**Top-M 设备路由**来限制通信范围, 并增加了**通信平衡损失**来均衡网络负载. 这标志着模型设计从纯粹的算法层面深入到了软硬件协同的系统工程层面.
    - **从 V2 到 V3**: 核心动机是**进一步提升算法的稳定性和推理的鲁棒性**, 并继续优化关键资源(如内存)的利用率. V3 将路由算法优化为更稳定的**Sigmoid + Softmax TopK**形式, 并采用更高效的**无辅助损失均衡策略**. 引入**序列级均衡**是为了应对推理时可能出现的极端情况. 同时, V3 引入了**MLA 注意力机制**, 旨在解决大模型在长序列推理中面临的 KV 缓存瓶颈, 显示了对模型全方位性能的极致追求.
+ **效果**: 这一系列演进使得 DeepSeek 团队能够成功地训练和部署从百亿到近七千亿参数的 MoE 模型, 并在性能上取得了领先. 每一步迭代都针对前一版本在更大规模下暴露出的问题, 提出了精准且有效的解决方案, 体现了在构建前沿大模型过程中的深刻洞察和卓越工程能力.

