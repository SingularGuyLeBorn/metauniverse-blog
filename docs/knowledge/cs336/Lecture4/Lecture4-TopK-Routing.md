### Top-K 路由机制详解



Top-K路由是现代混合专家模型(MoE)的基石. 它负责为每个输入的令牌(token)从海量的专家库中, 高效地选择出最相关的一小部分(K个)专家进行计算. 本笔记将深入解析其工作流程和关键的数学公式. 



#### 1. 核心目标与流程



Top-K路由的目标是计算出一个**稀疏的门控向量 `g_t`**, 该向量的维度等于专家的总数 `N`. 向量 `g_t` 中只有K个非零值, 这些值对应被选中的专家, 并作为权重来组合专家们的输出. 



整个流程可以分为三步:

1.  **计算亲和度分数 (Affinity Score)**:为输入令牌 `u_t` 和每个专家 `i` 计算一个标量分数 `s_i,t`, 表示它们之间的匹配程度. 

2.  **Top-K选择与门控 (Top-K Selection & Gating)**:从所有 `N` 个分数中选出最高的K个, 并根据这些分数生成门控向量 `g_t`. 

3.  **加权求和输出 (Weighted Sum Output)**:使用门控向量 `g_t` 对所有专家的输出进行加权求和. 



#### 2. 数学公式拆解



下面是描述Top-K路由的典型公式, 以DeepSeek V1/V2的实现为例:



1.  **最终输出 `h_t`**:

    `h_t = u_t + Σ (g_i,t * FFN_i(u_t))`

    - `h_t`: 该MoE层的最终输出. 

    - `u_t`: 输入令牌的向量表示(来自上一层的残差流). 

    - `FFN_i`: 第 `i` 个专家的前馈网络. 

    - `g_i,t`: 门控向量中对应第 `i` 个专家的权重值. 

    - `u_t + ...`: 残差连接, 是Transformer的标准组件. 



2.  **亲和度分数 `s_i,t`**:

    `s_i,t = Softmax_i(u_t^T * e_i)`

    - `e_i`: 一个可学习的向量, 可以理解为第 `i` 个专家的“身份嵌入”或“能力描述符”. 每个专家都有一个自己专属的 `e_i`. 

    - `u_t^T * e_i`: 计算输入令牌 `u_t` 和专家嵌入 `e_i` 的点积. 这与注意力机制中Query和Key的点积非常相似, 值越大代表亲和度越高. 

    - `Softmax_i(...)`: 对所有 `N` 个专家的点积得分进行Softmax归一化, 得到一组和为1的概率分布. 这个概率 `s_i,t` 代表了路由器认为令牌 `t` 应该被分配给专家 `i` 的“信心”或“意图”. 



3.  **门控向量 `g_i,t`**:

    `g_i,t = { s_i,t  if s_i,t ∈ TopK({s_j,t}, K),  else 0 }`

    - 这一步是实现**稀疏性**的关键. 

    - `TopK({s_j,t}, K)`: 从所有 `N` 个Softmax概率 `s_j,t` 中, 选出值最大的K个. 

    - `if ... else ...`: 如果某个专家的概率 `s_i,t` 入选了Top-K, 则其门控值 `g_i,t` 就等于该概率值 `s_i,t`; 否则, 其门控值 `g_i,t` 被强制设为0. 

    - 经过这一步, 向量 `g_t` 中就只有K个非零元素了. 



#### 3. 张量流动与维度分析



让我们追踪一下数据在路由过程中的形状变化:

- **输入 (`u_t`)**: 假设批大小为B, 序列长度为L, 隐藏层维度为D. 输入张量的形状为 `[B, L, D]`. 

- **专家嵌入 (`e_i`)**: 路由器的权重矩阵, 可以看作是 `N` 个专家的嵌入堆叠而成. 形状为 `[N, D]`. 

- **计算点积**: 将 `[B, L, D]` 和 `[N, D]` 的转置 `[D, N]` 相乘, 得到形状为 `[B, L, N]` 的亲和度分数矩阵. 矩阵中的每个元素 `(b, l, n)` 代表该批次中第 `b` 个样本的第 `l` 个令牌与第 `n` 个专家的原始匹配分数. 

- **Softmax**: 沿着最后一个维度(`N`)进行Softmax, 形状不变, 仍为 `[B, L, N]`. 现在每个 `(b, l, :)` 都是一个和为1的概率分布. 

- **Top-K与门控**: 对最后一个维度的 `N` 个概率值进行Top-K筛选. 这一步虽然概念上是筛选, 但在实现上通常是通过掩码(masking)完成的. 最终得到的门控张量 `g_t` 形状仍为 `[B, L, N]`, 但每一行 `(b, l, :)` 中最多只有K个非零值. 



#### 4. 实现上的微妙差异:Softmax的位置



值得注意的是, 不同模型在Softmax的应用上存在细微差别, 这会影响门控值的性质. 



- **Softmax-Before-TopK (如 DeepSeek V1/V2, Grok)**:

  - 这是我们上面详细解释的流程. 

  - **优点**: 门控值 `g_i,t` 是归一化的概率, 其总和(在Top-K个专家中)小于等于1. 这使得加权求和的输出在数值上更稳定. 

  - **缺点**: 经过Top-K筛选后, 剩余的门控值之和不再为1. 例如, 如果Top-2的概率是0.6和0.3, 它们的和是0.9. 



- **Softmax-After-TopK (如 Mixtral, DeepSeek V3)**:

  - 流程变为:1) 计算原始点积分数 -> 2) 选出分数最高的Top-K个专家 -> 3) **只对这K个专家的分数进行Softmax**. 

  - **优点**: 最终的门控值之和严格为1, 形成一个真正的加权平均. 

  - **缺点**: 这样做在概念上可能略显奇怪, 因为选择(Top-K)发生在归一化之前. 



在实践中, 这两种方法的性能差异并不显著. 因为后续的网络层(如LayerNorm)可以轻松地重新调整输出的尺度, 所以门控值之和是否严格为1并非至关重要. 这更多地反映了不同研究团队在架构设计上的美学偏好. 