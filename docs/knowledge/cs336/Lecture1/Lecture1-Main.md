# CS336: 从零开始构建语言模型 - 第一讲
## 1. 课程与教职员介绍
### 欢迎致辞与课程理念
- **Percy Liang (讲师)**: 欢迎大家来到 CS336！我对此课程充满期待,因为它将带领大家端到端地体验构建语言模型的完整流程,涵盖从数据、系统到建模的方方面面.
- **Tatsunori Hashimoto (联合讲师)**: 我和 Percy 花了很长时间思考,希望把真正有深度的技术教给学生. 我们坚信一个理念: “**唯有亲手从零构建,方能真正理解 (you gotta build it from scratch to understand it)**”. 这便是本课程的核心精神.
- **我(小卡拉米学生):** 我不能同意更多
![](%E6%95%99%E8%81%8C%E4%BB%8B%E7%BB%8D.png)
### 教学团队 (Teaching Assistants, TAs)
- **Rohith**: 他开玩笑说自己曾经挂掉这门课,但现在成为了你们的助教.
- **Neil**: 一位计算机系的博士三年级学生,研究兴趣在于模型、推理和合成数据.
- **Marcel**: 曾是去年课程中多个排行榜的冠军,是大家今年要挑战的目标.
### 课程更新
- 这是本课程第二次开设,规模扩大了50%,助教从两位增加到了三位.
- 一个重大的变化是: **所有课程视频都将上传至YouTube**,让全世界的学习者都能从零开始学习构建语言模型.
## 2. 为什么要开设这门课？
### 现状: 研究与技术的脱节
如果我们问 GPT-4 为什么要开设这样一门课,它会给出一些“促进基础理解、激发创新”之类的标准答案(低情商: 套话  typical generic blathers).
但真实的原因是: 我们正处在一场“危机”之中.
1. **研究人员与底层技术的脱节**: 八年前,AI研究者会亲手实现和训练自己的模型. 六年前,至少还会下载并微调 BERT 这样的模型. 而现在,许多人仅仅通过调用专有模型的API(Prompting)就能完成工作.
   这跟很多的CS技术一样 主动学习底层的 linux内核 操作系统 编译原理的人少之又少,反而大家都对新框架趋之若鹜. 
   放到大模型领域一样适用,好多人不懂Transformer架构,不知道MHA GQA到MLA的演进,不hi到vLLMs,SGLang等等Infra框架,但同样能够做研究(但是真像我说的啥也不懂估计也只能做Prompt工程了)
2. **抽象层的泄露 (Leaky Abstractions)**: 虽然更高层次的抽象能让我们完成更多工作,但这并非坏事. 然而,与编程语言或操作系统不同,语言模型的抽象层是“泄露”的,我们并不真正理解它的内部机制.
3. **基础研究的需求**: 许多根本性的研究突破,需要我们深入技术栈的底层,协同设计数据、系统和模型.**对技术的全面理解是进行基础研究的必要条件**.
因此,本课程的目标就是**为了推动基础研究的持续发展**,而我们的哲学是:**欲懂之,必造之 (to understand it, you have to build it)**.
### 挑战: 语言模型的工业化与规模限制
从零构建语言模型面临一个巨大挑战: **模型的工业化**.
- **恐怖的规模**: 据传 GPT-4 拥有 1.8 万亿参数,训练成本高达1亿美元. XAI 正在构建拥有20万个 H100 的计算集群,预计花费5000亿美刀(好像就是年初的星际之门计划,不过已经让DeepSeek基本击碎了). 这些数字对学术界来说是遥不可及的. 学术界一般都搞的是7B模型
- **技术保密**: 在OpenAI的技术报告中指出出于竞争和安全考虑,像 GPT-4 这样的前沿模型 (Frontier Models) 不会披露任何技术细节.
这意味着: **我们无法在课程中训练出自己的 GPT-4**. 我们将构建的是**小型语言模型 (small language models)**.
(但是可以做出自己的GPT2 By Andrew Karpathy 这位大佬真的是赛博活佛了
- [Let's build GPT: from scratch, in code, spelled out)](https://www.youtube.com/watch?v=kCc8FmEb1nY)
但这引出了一个新问题: **小型模型并不具有代表性(representative)的结论不一定能代表大型模型**.

| description | FLOPs / update | % FLOPs MHA | % FLOPs FFN | % FLOPs attn | % FLOPs logit |
| ----------- | -------------- | ----------- | ----------- | ------------ | ------------- |
| OPT setups  |                |             |             |              |               |
| 760M        | 4.3E+15        | 35%         | 44%         | 14.8%        | 5.8%          |
| 1.3B        | 1.3E+16        | 32%         | 51%         | 12.7%        | 5.0%          |
| 2.7B        | 2.5E+16        | 29%         | 56%         | 11.2%        | 3.3%          |
| 6.7B        | 1.1E+17        | 24%         | 65%         | 8.1%         | 2.4%          |
| 13B         | 4.1E+17        | 22%         | 69%         | 6.9%         | 1.6%          |
| 30B         | 9.0E+17        | 20%         | 74%         | 5.3%         | 1.0%          |
| 66B         | 9.5E+17        | 18%         | 77%         | 4.3%         | 0.6%          |
| 175B        | 2.4E+18        | 17%         | 80%         | 3.3%         | 0.3%          |
- **例子1: 计算量分布 (FLOPs Distribution)**
  - 在小型模型中,注意力层 (Attention) 和前馈网络层 (MLP) 的计算量大致相当.
  - 但在1750亿参数的大型模型中,MLP 的计算量占据了绝对主导.
  - 这意味着,如果在小模型上花费大量精力优化 Attention,可能是在优化一个在大规模下无关紧要的部分.
- **例子2: 涌现能力 (Emergent Behavior)**
  - 研究表明,许多能力(如上下文学习 In-context learning)只有在模型的训练计算量达到某个阈值后才会“涌现”.
  - 如果只在小规模下实验,你可能会得出“语言模型没用”的错误结论.
  > <b><span style="color:rgba(244,63,94,1)">关于涌现,有一个很有意思的思考</span></b>
  > 
  > 前置结论: 高维空间更自由,所以梯度下降并不容易陷入局部最优,当维度足够高,几乎必然能达到全局最优
  > 
  > 联想到数学方面的一些知识,越高维的空间里面向量越稀疏,球体的体积越集中在表面(可能也越平滑)
  > 
  > 所以从任意一点出发沿着梯度方向走几乎一定能走到全局最优
  > 
  > 拓扑学的绳节理论中也有类似现象: 可以证明,绳子打结仅仅会发生在包括3维在内的低维空间中,当空间维度足够高时绳子永远不会打成死结,任意缠绕的线团只需要拉紧两端,就能直接解开. 
  > 
  > 还听说过一个理论,解释为什么宇宙是3维而不是更高维的: 维度越高,两个粒子相遇的概率越低,导致难以发生相互作用,也就难以形成复杂结构. 换句话说,高维空间中想要困住一个粒子很难. 
  > 
  > 还有篇凝聚态物理讲晶体结构优化的,提出一个方法: 通过增加额外维度,能避免优化时卡在局域最小处,使得体系可以通过额外的自由度绕过势垒,更快达到全局最小,这和本视频提到的“虫洞”观点不谋而合. 
  > 
  > 几个完全不同领域的研究会出现相似的规律,真的很有意思！
  > 
  > —————————————————
  > 
  > 然后对大模型的`涌现`有一些想法,有没有可能是全局最优就是能带来最强的智力,只是在模型不够大的时候梯度下降并没有真正走到全局最优. 而模型足够大,空间维度足够高梯度下降走到了全局最优,然后智力就出现了. 
  > 
  > 所以不是大规模模型涌现出智能,而是大规模模型能让本就能带来智能训练方法实现了它的设计目的？
## 3. 我们能从这门课学到什么？
尽管存在上述挑战,我们依然能学到宝贵的知识. 知识可以分为三类:
1. **机制 (Mechanics)**:**这部分可以完全教会**.
   - 事物如何工作的原理. 例如,什么是 Transformer,模型并行化 (model parallelism) 如何高效利用 GPU. 这些是构建模型的“原材料”.
2. **思维模式 (Mindset)**:**这部分至关重要,也能够教会**.
   - 我们将培养一种核心思维: **尽可能压榨硬件性能,并严肃对待规模化 (scaling)**.
   - 许多技术(如 Transformer)的组件早已存在,但正是 OpenAI 所引领的“规模化思维”催生了新一代AI模型.
3. **直觉 (Intuitions)**:**这部分只能部分教会**. 这大概就是ilya这样的神人和我们普通人的区别所在
   - 关于“哪些数据和建模决策 (modeling decisions) 能带来好模型 (good models)”的直觉.
   - 这是因为在小规模上有效的架构和数据集,在大规模上可能并非如此.
   - 有时候,最佳实践来自于实验而非理论. 例如,SwiGLU 这个非线性激活函数效果很好,但论文作者在结论中坦言: “除了神恩浩荡 (divine benevolence),我们无法提供任何解释(extend to our understanding). ”
## 4. 效率与“惨痛的教训” (The Bitter Lesson)
许多人对 Rich Sutton 的“惨痛的教训”有一个误解,认为“规模就是一切,算法不重要”.
- **正确的解读**:**在规模化场景下的算法 (algorithms at scale) 才是最重要的**.
- **核心公式**: `模型准确率 (Accuracy) = 算法效率 (Efficiency) × 投入资源 (Resources)`.
- **效率的重要性**: 在投入数亿美元训练模型时,效率至关重要,任何浪费都是不可承受的.
- **算法的进步**: 一篇2020年 OpenAI 的论文指出,从2012到2019年,将 ImageNet 训练到同等准确率所需的算力,由于算法效率的提升,降低了44倍.**这个速度甚至超过了摩尔定律**.
因此,本课程的核心框架问题是:
> **在给定的计算和数据预算下,你能构建出的最佳模型是什么？**
> 
> - 它在任何规模下都很重要
作为研究者,我们的目标就是**最大化算法效率**.
## 5. 语言模型简史与现状
- **早期**: 语言模型的概念可追溯到香农 (Shannon) 估算英语熵的工作. 在AI领域,它曾是机器翻译、语音识别等大系统的一个组件. 值得注意的是,早在2007年,Google 就在2万亿个 token 上训练了 5-gram 模型,这个 token 数量比 GPT-3 还多. 但是并没有什么有趣的事情发生
- **深度学习革命的组件**:
  - 2003年: 第一个神经网络语言模型 (Bengio 团队).
  - 后续发展: Seq2Seq 模型、Adam 优化器、注意力机制 (Attention).
  - 2017年: **Transformer** 诞生 (论文: *Attention Is All You Need*).
  - 系统层面: 混合专家模型 (Mixture of Experts) 和模型并行化 (Model Parallelism) 的研究也已成熟.
- **预训练与微调时代**: ELMO, BERT, T5 等模型引领了“基础模型 (foundation models)”的潮流.
- **规模化时代**: OpenAI 整合了上述技术,并凭借卓越的工程能力和对规模化定律的信仰,推出了 GPT-2 和 GPT-3,开启了新纪元.
- **开放模型的兴起**: 随着闭源模型 (closed models) 的成功,社区也涌现出大量开源/开放模型 (open models),如 EleutherAI、Meta (LLaMA)、Bloom、DeepSeek 等.
- **“开放”的层次**:
  1. **闭源模型 (Closed Models)**: 如 GPT-4,不提供任何细节.
  2. **开放权重模型 (Open-weight Models)**: 提供模型权重,但可能不提供数据细节.
  3. **开源模型 (Open-source Models)**: 提供权重、数据和详尽的论文.
## 6. 课程安排与后勤 (Logistics)
- **课程网站**: 所有资料都在线上.
- **工作量**: 这是一门5个学分的课程,但工作量巨大. 有学生评价: “**第一次作业的工作量约等于 CS224n 的五次作业加上期末项目**”.
- **适合人群**:
  - 对理解事物底层原理有执念的人.
  - 希望在研究工程能力和大规模机器学习系统构建方面实现飞跃的人.
- **不适合人群**:
  - 想在本学期完成其他研究项目的人.
  - 只想学习最新最热技术(而不是花时间调试BPE)的人.
  - 将此课作为学习构建LLM应用的入门课的人. (建议先从 Prompting 和微调开始)
- **作业 (Assignments)**:
  - 共5次作业,**不提供脚手架代码 (scaffolding code)**,你将从一个空文件开始.
  - 我们提供单元测试 (unit tests) 来检查正确性.
  - 你需要自己做软件设计决策.
  - **策略**: 先在本地用小数据集实现和调试,再到集群上进行基准测试.
  - 有些作业会有一个leaderboard排行榜,谁能在有限资源下让perplexity降到最低
    > [如何理解PPL](prerplexity.md)
- **计算集群**:
  - 感谢 Together AI 提供的 H100 GPU 集群.  [集群使用指南](https://docs.google.com/document/d/1BSSig7zInyjDKcbNGftVxubiHlwJ-ZqahQewIzBmBOo/edit)
  - 请尽早开始作业,因为截止日期前集群会非常拥堵.
- **AI 编程工具**:
  - Copilot 等工具可能会剥夺你的学习体验.
  - 你可以审慎使用,但后果自负,你需要为自己的学习负责.
## 7. 课程内容概览: 五大支柱
本课程围绕**效率**这一核心原则,分为五大支柱,探讨在给定的硬件和数据预算下如何做出最优的设计决策.
如果有一个CommomCrawl(一个大型的开放式网络爬虫数据集),一个网络数据转储和32块H100,然后两周时间,该怎么做?
本课程将其分为五个阶段
![](%E4%BA%94%E4%B8%AA%E9%98%B6%E6%AE%B5.png)
> “网络转储”(Web Dump)是指**对互联网或其中一部分内容进行大规模抓取、复制,并将其保存为一个可供离线访问、分析或处理的数据集合**
> 
> 可以将其理解为: 
> 
> - **“网页快照”或“网页档案”的集合**: 它不是单个网页的截图,而是整个网页的源代码、文本、链接等数据的大规模集合. 
> - **数据的“倾倒”**: 就像数据库转储(database dump)是将数据库的所有数据复制出来一样,网络转储就是把网络上的数据“倾倒”出来,形成一个文件或一系列文件. 
> - **通常是原始的、未处理的数据**: 网络转储的数据通常是非常原始的,包含HTML标签、脚本、广告、重复内容、低质量文本等各种噪音. 
> 
> **Common Crawl 就是一个典型的“网络转储”实例.** Common Crawl 组织定期对整个互联网进行大规模爬取,并将抓取到的网页内容打包成巨大的数据集文件(这些文件就是“网络转储”),免费提供给公众下载和使用. 
### 支柱一: 基础 (Basics)
- **目标**: 搭建一个可以工作的完整训练pipeline.
- **内容**:
  - **分词器 (Tokenizer)**: 实现字节对编码 (Byte-Pair Encoding, BPE). [tokenizer详解](Lecture1-Tokenizer.md)
  - **模型架构 (Model Architecture)**: 实现 Transformer 及其现代改进版(如 SwiGLU 激活函数、RoPE 位置编码、RMSNorm 归一化).
  - **训练 (Training)**: 实现 AdamW 优化器(后续考虑补上Muon)、学习率调度和完整的训练循环.
- **作业一**:
  - 实现上述所有组件.
  - 在给定的 H100 算力预算(90分钟)下,参加 OpenWebText 数据集上的困惑度 (perplexity) 排行榜.
  - ps:可以使用pytorch,但不能使用pytorch的transformer实现(我的想法是先用pytorch实现,后面直接用numpy+pandas实现)
### 支柱二: 系统 (Systems)
- **目标**: 压榨硬件的极致性能.
  在完成作业一后,已经有能力去训练一个transformer模型了,这部分主要探讨如何优化它
- **内容**:
  - **核函数 (Kernels)**: 深入理解 GPU 架构,学习如何通过算子融合 (fusion) 和分块 (tiling) 等技术减少数据搬运,并使用 Triton 编写高效的核函数.
  - **并行化 (Parallelism)**: 学习数据并行 (data parallelism)、张量并行 (tensor parallelism) 等技术,在多GPU上高效训练.
  - **推理 (Inference)**: 学习推理过程中的 Prefill 和 Decode 阶段,以及如何通过推测解码 (speculative decoding) 等技术进行加速.
- **作业二**:
  - 实现自定义的 Triton kernel.
  - 实现数据并行化.
  - 学会使用性能分析工具 (profiler) 来定位瓶颈.
### 支柱三: 缩放定律 (Scaling Laws)
- **目标**: 通过小规模实验,预测大规模训练时的超参数和性能.
  在给定的算力下,如何选择模型的大小？(这个问题已经有了广泛被认可的答案)
  - [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361) By OpenAI 2020
  - [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556) By DeepMind 2022
  - 每个算力量级都有最优的参数量,当把图画出来的时候,惊人的发现他们是线性关系
**[Missing Image: Scaling Laws - THxhih5qE18mGFk.png]**
- **内容**:
  - **Chinchilla 最优**: 学习在给定的计算预算下,如何平衡模型大小 (model size) 和训练数据量,以达到最优性能.
  - **经验法则**: `模型参数量 × 20 ≈ 最佳训练 Token 数`  但局限性是这没考虑到推理成本
- **作业三**:
  - 在一个模拟的“训练API”上,用有限的 FLOPs 预算进行实验.
    这个API需要指定架构,batchsize,参数量,超参数等等,api会返回决策所产生的损失
  - 根据实验数据拟合自己的缩放定律.
  - 提交你对更大规模训练的最佳超参数的预测,参加排行榜.
### 支柱四: 数据 (Data)
- **目标**: 理解数据如何决定模型的能力.
  模型的功能完全(或者说大部分)是由数据决定的,如果用多语言数据训练就会有多语言能力等等
**[Missing Image: Data Sources - bKwzlsnbBVWwsMN.png]**
数据来源多种多样,这张图是四年前的,但依然适用
- **内容**:
  - **评估 (Evaluation)**: 学习如何评估模型,包括困惑度 (Perplexity)、标准化测试 (如 MMLU) 和对指令遵循能力的评估.
  - **数据策管 (Data Curation)**: 数据并非凭空而来. 我们将学习如何处理原始网络数据 (如 Common Crawl),包括:
    - 从 HTML 中提取高质量文本.
      重点在于你如何能既保留内容和部分结构,又不简单地对html进行过滤,通常会训一个分类器来决定
    - 数据过滤 (filtering) 和去重 (deduplication).
    - 处理有害内容.
- **作业四**:
  - 处理原始的 Common Crawl 数据.
  - 训练分类器进行数据筛选.
  - 在给定的 Token 预算下,优化模型在测试集上的困惑度,参加排行榜.
### 支柱五: 对齐 (Alignment)
- **目标**: 让预训练好的基础模型 (base model) 变得有用、听话且安全.
- **内容**:
  - **对齐的三个方面**: ①遵循指令;②控制生成风格;③安全(拒绝有害回答).
  - **监督式微调 (Supervised Fine-Tuning, SFT)**: 使用高质量的“指令-回答”对来微调模型,这部分和预训练区别不大
  - **从反馈中学习 (Learning from Feedback)**:
    是利用更轻量级的标注形式,不用找人坐下来标数据
    - **数据**: 使用偏好数据 (preference data),即标注者判断模型生成的两个或多个回答中哪个更好.
    - **算法**: 学习 DPO (Direct Preference Optimization) 和 GRPO (Group-Relative Preference Optimization) 等比传统强化学习更简单高效的算法.(DeepSeek团队移除了PPO的价值函数变成了GRPO)
      一开始算法是PPO,但如果只有偏好数据那么DPO是效果非常不错的,包括更轻量的GRPO效果也不错
      题外话:PPO至今都没有发论文,还在Arxiv上挂着,但这并不妨碍它成为LLM时代最成功的RL算法,即便它对于资源的消耗也是惊人的.但是DPO和GRPO这类轻量化算法也非常有效,这算不算是一种智力层面的摩尔定律,同样的数据,换一个训练方法,就能用更少的算力训出相同能力的模型
- **作业五**:
  - 实现 SFT、DPO 和 GRPO,以及评估环节
## 8. 第一单元深入探讨: 分词 (Tokenization)
分词是将原始文本字符串 (string) 转换为整数序列 (sequence of integers) 的过程,也是模型处理文本的第一步.
- [Andrew Karpathy - Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
### 尝试一: 基于字符的分词 (Character-based)
- **方法**: 将每个 Unicode 字符映射到一个整数.
- **优点**: 非常简单.
- **缺点**:
  1. **词汇表过大**: Unicode 字符非常多.
  2. **效率低下**: 为罕见字符分配了独立的 token,浪费了词汇表空间.
  3. **压缩率低**: 序列长度很长.
### 尝试二: 基于字节的分词 (Byte-based)
- **方法**: 将字符串编码为 UTF-8 字节序列,每个字节 (0-255) 是一个 token.
- **优点**: 词汇表大小固定为256,非常小巧,没有未知 token.
- **缺点**:
  1. **压缩率极差 (1.0)**: 每个字节都变成一个 token,导致序列非常长.
  2. **计算效率低**: Transformer 的注意力机制是序列长度的二次方复杂度,长序列会带来巨大的计算开销.
### 尝试三: 基于单词的分词 (Word-based)
- **方法**: 用正则表达式等方式将文本切分成单词.
- **优点**: 压缩率高,符合直觉.
- **缺点**:
  1. **词汇表无限大**: 总会遇到词汇表中没有的新词(Out-of-Vocabulary, OOV).
  2. **处理 "unk" 标记**: 需要用一个特殊的 `[UNK]` 标记来表示未知词,处理起来很麻烦.
### 最终选择: 字节对编码 (Byte Pair Encoding, BPE)
BPE 是一种非常古老(1994年)的数据压缩算法,后来被引入 NLP 领域,并由 GPT-2 发扬光大.
- [A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM) By Philip Gage 1994
- [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)  tiktoken is a fast BPE tokeniser for use with OpenAI's models.
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) By OpenAI
- **核心思想**:**不再预设分词规则,而是从数据中学习**. 它有机地将语料中频繁出现的字符序列合并成一个 token.
- **BPE 训练算法**:
  1. **初始化**: 将文本转换为字节序列,初始词汇表就是所有单个字节(0-255).
  2. **迭代合并**:
     a.  统计当前序列中所有相邻 token 对的出现频率.
     b.  找到频率最高的 token 对(例如 `('t', 'h')`).
     c.  将这个 token 对合并成一个新的 token(例如 `'th'`),并将其加入词汇表.
     d.  在整个文本序列中,将所有出现的 `('t', 'h')` 替换为新的 `'th'` token.
  3. **重复**: 重复上一步骤,直到达到预设的词汇表大小.
- **优点**:
  - **自适应**: 能根据语料库自动学习词汇.
  - **压缩率和序列长度的平衡**: 常见词(如 "language")成为一个 token,不常见词(如 "tokenization")被拆分成几个子词(如 "token" + "ization"),既避免了 OOV 问题,又控制了序列长度.
  - **可逆**: 没有信息损失.
- **总结**: BPE 是一个非常有效的启发式算法. 尽管我们都希望未来能有直接处理原始字节的高效模型架构,但在那一天到来之前,我们仍需与分词技术打交道.
## 9. 课程预告
今天的课程到此结束. 下次课我们将深入 **PyTorch 的细节**,并关注**资源核算 (resource accounting)**,教会大家如何精确地分析模型训练中每一份计算资源(FLOPs)的去向. 我们下次见！
## 10. 拓展阅读
-  [Milti-Token Prediction](Lecture1-MTP.md)